"""
Probe Query Tool - ç®€åŒ–ç‰ˆæœ¬

ç›´æŽ¥ä½¿ç”¨å‡½æ•°ï¼Œä¸ç»§æ‰¿å¤æ‚çš„ BaseTool
"""

import time
from typing import Any, Dict, Optional
from .models import ProbeRequest, ProbeResponse, QueryStage, PrecisionLevel


class ProbeQueryTool:
    """
    Probe æŸ¥è¯¢å·¥å…·ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    
    ä¸ç»§æ‰¿ AgenticX BaseToolï¼Œç›´æŽ¥å®žçŽ°åŠŸèƒ½
    """
    
    def __init__(
        self,
        database_connector=None,
        memory_store=None,
        llm_provider=None
    ):
        """åˆå§‹åŒ– Probe Tool"""
        self.name = "probe_query"
        self.description = "æ‰§è¡Œæ™ºèƒ½åŒ–çš„æ•°æ®åº“æŸ¥è¯¢ï¼Œç†è§£æŸ¥è¯¢æ„å›¾å’Œé˜¶æ®µ"
        self.database = database_connector
        self.memory_store = memory_store
        self.llm_provider = llm_provider
        self.query_count = 0
        self.cache_hits = 0
    
    async def aexecute(
        self,
        natural_query: str,
        stage: str = "full_validation",
        precision: str = "exact",
        context: str = "",
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥æ‰§è¡Œ Probe æŸ¥è¯¢
        
        Args:
            natural_query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            stage: æŸ¥è¯¢é˜¶æ®µ
            precision: ç²¾åº¦çº§åˆ«
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
            
        Returns:
            Dict: æŸ¥è¯¢ç»“æžœ
        """
        start_time = time.time()
        
        # åˆ›å»º Probe è¯·æ±‚
        probe_request = ProbeRequest(
            natural_query=natural_query,
            stage=QueryStage(stage),
            precision=PrecisionLevel(precision),
            context=context
        )
        
        try:
            # 1. æ£€æŸ¥è¯­ä¹‰ç¼“å­˜
            if self.memory_store:
                cached = await self._check_semantic_cache(probe_request)
                if cached:
                    self.cache_hits += 1
                    cached.was_cached = True
                    cached.execution_time = time.time() - start_time
                    return cached.model_dump()
            
            # 2. æ ¹æ®é˜¶æ®µä¼˜åŒ–æŸ¥è¯¢
            probe_request = self._optimize_for_stage(probe_request)
            
            # 3. æ‰§è¡ŒæŸ¥è¯¢
            response = await self._execute_query(probe_request)
            
            # 4. ç”Ÿæˆå»ºè®®
            response = self._generate_suggestions(response, probe_request)
            
            # 5. ç¼“å­˜ç»“æžœ
            if self.memory_store and response.success:
                await self._cache_result(probe_request, response)
            
            self.query_count += 1
            response.execution_time = time.time() - start_time
            
            return response.model_dump()
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            execution_time = time.time() - start_time
            return ProbeResponse(
                request_id=probe_request.request_id,
                success=False,
                error=str(e),
                error_type=type(e).__name__,
                execution_time=execution_time,
                suggestions=self._generate_error_suggestions(e)
            ).model_dump()
    
    def execute(self, **kwargs) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œ"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.aexecute(**kwargs))
    
    async def _check_semantic_cache(self, request: ProbeRequest) -> Optional[ProbeResponse]:
        """æ£€æŸ¥è¯­ä¹‰ç¼“å­˜"""
        if not self.memory_store:
            return None
        
        similar_queries = await self.memory_store.find_similar_probes(
            request.natural_query,
            threshold=0.8
        )
        
        if similar_queries:
            return similar_queries[0]
        
        return None
    
    def _optimize_for_stage(self, request: ProbeRequest) -> ProbeRequest:
        """æ ¹æ®æŸ¥è¯¢é˜¶æ®µä¼˜åŒ–è¯·æ±‚"""
        if request.stage == QueryStage.METADATA_EXPLORATION:
            request.terminate_early = True
            request.max_rows = request.max_rows or 10
            if request.precision == PrecisionLevel.EXACT:
                request.precision = PrecisionLevel.SAMPLE
        
        elif request.stage == QueryStage.SOLUTION_FORMULATION:
            request.max_rows = request.max_rows or 100
            if request.precision == PrecisionLevel.EXACT:
                request.precision = PrecisionLevel.APPROXIMATE
        
        return request
    
    async def _execute_query(self, request: ProbeRequest) -> ProbeResponse:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰"""
        # æ¨¡æ‹Ÿæ•°æ®
        mock_data = [
            {"id": 1, "name": "Product A", "sales": 150},
            {"id": 2, "name": "Product B", "sales": 200},
            {"id": 3, "name": "Product C", "sales": 100},
        ]
        
        rows = 3 if request.precision == PrecisionLevel.SAMPLE else 10
        if request.max_rows:
            rows = min(rows, request.max_rows)
        
        return ProbeResponse(
            request_id=request.request_id,
            success=True,
            data=mock_data[:rows],
            executed_sql=request.sql_query or "SELECT * FROM mock_table",
            rows_returned=len(mock_data[:rows]),
            rows_scanned=100,
            actual_precision=request.precision,
            confidence=0.95 if request.precision != PrecisionLevel.EXACT else 1.0,
            is_approximate=request.precision != PrecisionLevel.EXACT
        )
    
    def _generate_suggestions(self, response: ProbeResponse, request: ProbeRequest) -> ProbeResponse:
        """ç”Ÿæˆå»ºè®®"""
        suggestions = []
        
        if request.stage == QueryStage.METADATA_EXPLORATION:
            suggestions.append("âœ… å…ƒæ•°æ®æŽ¢ç´¢å®Œæˆï¼Œå¯ä»¥å¼€å§‹åˆ¶å®šè§£å†³æ–¹æ¡ˆ")
        
        elif request.stage == QueryStage.SOLUTION_FORMULATION:
            suggestions.append("ðŸ’¡ å»ºè®®è¿›è¡Œå®Œæ•´éªŒè¯ä»¥èŽ·å–ç²¾ç¡®ç»“æžœ")
        
        if request.precision != PrecisionLevel.EXACT:
            suggestions.append(f"âš ï¸ å½“å‰ä½¿ç”¨{request.precision.value}ç²¾åº¦ï¼Œå¦‚éœ€ç²¾ç¡®ç»“æžœè¯·ä½¿ç”¨ exact ç²¾åº¦")
        
        response.suggestions = suggestions
        return response
    
    def _generate_error_suggestions(self, error: Exception) -> list:
        """ç”Ÿæˆé”™è¯¯å»ºè®®"""
        return ["æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢è¯­å¥å’Œæ•°æ®åº“è¿žæŽ¥"]
    
    async def _cache_result(self, request: ProbeRequest, response: ProbeResponse):
        """ç¼“å­˜æŸ¥è¯¢ç»“æžœ"""
        if not self.memory_store:
            return
        
        await self.memory_store.cache_probe_result(
            probe_request=request.model_dump(),
            probe_response=response.model_dump()
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–å·¥å…·ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_queries": self.query_count,
            "cache_hits": self.cache_hits,
            "cache_hit_rate": self.cache_hits / self.query_count if self.query_count > 0 else 0,
            "redundancy_savings": f"{(self.cache_hits / self.query_count * 100):.1f}%" if self.query_count > 0 else "0%"
        }

